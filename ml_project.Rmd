---
title: "Coursera Machine Learning Project"
author: "Justin L"
date: "Saturday, April 11, 2015"
output: html_document
---

##Introduction

This project utilizes weight lifting exercise data from the "Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements" research (see citation below).

From the research paper: "Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)."

The goal of this paper is to use a set of training data in order to develop a machine learning model in order to correctly predict the type of bicep curl (classes A through E) performed on a set of testing data.

##Model Used
For this project, the decision was made due use a Random Forest model due to its robustness and accuracy.  Although a 25 sample Bootstrapping resampling is used as a default in the caret implementation of the random forest model in the train function, it was found that extensive memory and time requirements were necessary for the default implementation.

Because my system lacked the necessary RAM, the decision was made to use 3-fold cross validation prior to training the data with the Random Forest Model.

##Data cleaning
It was observed that many of the row data in several columns of the training data was either blank or NA.  Due to the lack of useful prediction capability of those columns, these columns were removed from the data set.  Also removed were the user name and timestamp data.  That resulted in 53 columns of data for a total of 52 predictors (plus the classification).


```{r, echo=FALSE}
#read in the data
setInternet2(use = TRUE)
train.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(train.url, "./pml-training.csv")
download.file(test.url, "./pml-testing.csv")
training.data <- read.csv("./pml-training.csv")
testing.data <- read.csv("./pml-testing.csv")
```

```{r, echo=FALSE}
#clean the training data (19216 is the number of NAs in many columns, so just remove them)
training.data[training.data==""] <- NA
training.data <- training.data[,colSums(is.na(training.data))<19216]
drop.cols <- c("X","user_name","cvtd_timestamp", "raw_timestamp_part_1","raw_timestamp_part_2", "new_window", "num_window")
training.data <- training.data[,!(names(training.data) %in% drop.cols)]
training.data$classe <- factor(training.data$classe)
#clean testing data
testing.data[testing.data==""] <- NA
#get rid of NA columns
testing.data <- testing.data[,colSums(is.na(testing.data))<20]
testing.data <- testing.data[,!(names(testing.data) %in% drop.cols)]
```
##Plotting the data
Taking a look at the initial data, you can see a relationship between total acceleration from a sensor and the resulting classification.  For example, for the "E" classification (throwing the hips to the front), there is greater hip acceleration than any other classification.  Other acceleration data from other sensors (forearm, arm, dumbbell) show similar results.  See the boxplots below.
```{r, echo=FALSE}
library(ggplot2)
library(grid)
library(gridExtra)
p1 <- qplot(classe, total_accel_belt, data=training.data, fill=classe, geom=c("boxplot", "jitter"))
p2 <- qplot(classe, total_accel_forearm, data=training.data, fill=classe, geom=c("boxplot", "jitter"))
p3 <- qplot(classe, total_accel_arm, data=training.data, fill=classe, geom=c("boxplot", "jitter"))
p4 <- qplot(classe, total_accel_dumbbell, data=training.data, fill=classe, geom=c("boxplot", "jitter"))
grid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)
```


##Training the data
The training set provided was further divided into a sub-training set (75%) and a sub-testing set (25%).

```{r, echo=FALSE}
##create data partition with training and testing data
library(caret)
library(randomForest)
train.part <- createDataPartition(y=training.data$classe, p=0.75, list=FALSE)
training <- training.data[train.part,]
testing <- training.data[-train.part,]
#train data
set.seed(1000)
```

The training set was then run through the Random Forest training model with 3-fold cross validation.  The resulting model had an in-sample accuracy of over 99%.

```{r, echo=TRUE}
#set up training control function with cross validation
fitControl <- trainControl(method = "cv", number = 3)
modelFit <- train(classe ~., data=training, trControl=fitControl, 
                  method="rf", tuneGrid = data.frame(mtry = 6), ntree=100)
modelFit
```

The resulting model was then used to test the sub-testing set derived from the training data.  The "out-of-sample" accuracy was still over 99%, which I felt was adequate enough to apply to the final test data.  While this out-of-sample is still within the training data set, the 95% confidence interval estimates that the out-of-sample error for the test set would still be over 99%.

```{r, echo=TRUE}
test.predict <- predict(modelFit, testing)
confusionMatrix(test.predict, testing$classe)
```

The final step was to apply the model to the final test data.
```{r, echo=TRUE}
#test on final testing set
final.predict <- predict(modelFit,testing.data)
final.predict
```

These final predictions were then assembled into text files and submitted to the Coursera website.  All 20 test cases were classified correctly.

###References

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.

Read more: http://groupware.les.inf.puc-rio.br/har#sbia_paper_section#ixzz3YRDLWxEV
